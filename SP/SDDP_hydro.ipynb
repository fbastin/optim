{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDDP: electric generation examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAST: the hydro-thermal problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on https://odow.github.io/SDDP.jl/latest/examples/FAST_hydro_thermal/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We a consider a toy hydro-thermal example, adapted from https://github.com/leopoldcambier/FAST/blob/master/examples/hydro%20thermal/hydro_thermal.m, where we can produce eletricity either with a thermal plant, either with a hydroelectric plant. The uncertainty is the amount of rainfall at each stage. Rainfall can either be high (10) or small (2), with a uniform probability. At each stage, the demand is equal to 6, and can be satisfied either by burning some fuel, with a price equal to 5 per demand unit, either by using some water, at a negligeable cost. From stage to stage, water can been stored in the reservoir, but there is a tank limit equal to 8. We denote the stored water level at the end of stage 1 by $x_1$ and the used quantity at stage $1$ by $y_1$. The quantity of purchased fuel at stage 1 is denoted by $p_1$, and the initial water quantity in the reservoir is 6.\n",
    "\n",
    "The problem at stage 1 is then\n",
    "\\begin{align*}\n",
    "  \\min\\ & 5 p_1 + V(x_1) \\\\\n",
    "  \\text{s.t. } & x_1 \\leq 8 \\\\\n",
    "  & x_1 \\leq 6 - y_1 \\\\\n",
    "  & p_1 + y_1 \\geq 6\n",
    "\\end{align*}\n",
    "where $V(x_1)$ is the expected second-stage cost, expressed at\n",
    "\\begin{align*}\n",
    "V(x_1) &= 30 - E\\left[5 \\min \\{ x_1 + \\xi, 6 \\} \\right] \\\\ \n",
    "       &= 30 - 0.5 ( 5 \\min \\{ x_1+2,6 \\} + 5 \\min \\{ x_1+10,6 \\} ) \\\\\n",
    "       &= 15 - 5/2 \\min \\{ x_1+2, 6 \\}\n",
    "\\end{align*}\n",
    "\n",
    "The solution at stage 1 can then trivially be found to be\n",
    "$$\n",
    "   x_1 = 0,\\ y_1 = 6,\\ p_1 = 0\n",
    "$$\n",
    "for an expected cost of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SDDP, HiGHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg; Pkg.add(\"SDDP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A policy graph with 2 nodes.\n",
       " Node indices: 1, 2\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SDDP.PolicyGraph(\n",
    "    SDDP.LinearGraph(2),  # Construct the graph with 2 stages\n",
    "    bellman_function = SDDP.BellmanFunction(lower_bound = 0.0),\n",
    "    optimizer = HiGHS.Optimizer,\n",
    ") do sp, t\n",
    "    # At each stage, the model state is the reservoir level x.\n",
    "    @variable(sp, 0 <= x <= 8, SDDP.State, initial_value = 0.0)\n",
    "    @variables(sp, begin\n",
    "            y >= 0\n",
    "            p >= 0\n",
    "            ξ\n",
    "        end)\n",
    "    @constraints(sp, begin\n",
    "            p + y >= 6\n",
    "            x.out <= x.in - y + ξ\n",
    "        end)\n",
    "    \n",
    "    # We define the rainfall levels.\n",
    "    # The level is deterministic at stage 1, but can take one of two values at subsequent stages.\n",
    "    RAINFALL = (t == 1 ? [6] : [2, 10])\n",
    "    \n",
    "    SDDP.parameterize(sp, RAINFALL) do ω\n",
    "        JuMP.fix(ξ, ω)\n",
    "    end\n",
    "    @stageobjective(sp, 5 * p)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A JuMP Model\n",
       "Minimization problem with:\n",
       "Variables: 18\n",
       "Objective function type: AffExpr\n",
       "`AffExpr`-in-`MathOptInterface.EqualTo{Float64}`: 2 constraints\n",
       "`AffExpr`-in-`MathOptInterface.GreaterThan{Float64}`: 3 constraints\n",
       "`AffExpr`-in-`MathOptInterface.LessThan{Float64}`: 3 constraints\n",
       "`VariableRef`-in-`MathOptInterface.EqualTo{Float64}`: 4 constraints\n",
       "`VariableRef`-in-`MathOptInterface.GreaterThan{Float64}`: 12 constraints\n",
       "`VariableRef`-in-`MathOptInterface.LessThan{Float64}`: 5 constraints\n",
       "Model mode: AUTOMATIC\n",
       "CachingOptimizer state: EMPTY_OPTIMIZER\n",
       "Solver name: HiGHS"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det = SDDP.deterministic_equivalent(model, HiGHS.Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min 5 p#1 + 2.5 p#2 + 2.5 p#2\n",
      "Subject to\n",
      " x_out#1 - x_in#2 == 0.0\n",
      " x_out#1 - x_in#2 == 0.0\n",
      " y#1 + p#1 >= 6.0\n",
      " y#2 + p#2 >= 6.0\n",
      " y#2 + p#2 >= 6.0\n",
      " -x_in#1 + x_out#1 + y#1 - ξ#1 <= 0.0\n",
      " -x_in#2 + x_out#2 + y#2 - ξ#2 <= 0.0\n",
      " -x_in#2 + x_out#2 + y#2 - ξ#2 <= 0.0\n",
      " x_in#1 == 0.0\n",
      " ξ#1 == 6.0\n",
      " ξ#2 == 2.0\n",
      " ξ#2 == 10.0\n",
      " x_out#1 >= 0.0\n",
      " y#1 >= 0.0\n",
      " p#1 >= 0.0\n",
      " _[6]#1 >= 0.0\n",
      " x_out#2 >= 0.0\n",
      " y#2 >= 0.0\n",
      " p#2 >= 0.0\n",
      " _[6]#2 >= 0.0\n",
      " x_out#2 >= 0.0\n",
      " y#2 >= 0.0\n",
      " p#2 >= 0.0\n",
      " _[6]#2 >= 0.0\n",
      " x_out#1 <= 8.0\n",
      " x_out#2 <= 8.0\n",
      " _[6]#2 <= 0.0\n",
      " x_out#2 <= 8.0\n",
      " _[6]#2 <= 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running HiGHS 1.5.3 [date: 1970-01-01, git hash: 45a127b78]\n",
      "Copyright (c) 2023 HiGHS under MIT licence terms\n",
      "Presolving model\n",
      "6 rows, 7 cols, 12 nonzeros\n",
      "6 rows, 7 cols, 12 nonzeros\n",
      "Presolve : Reductions: rows 6(-2); columns 7(-11); elements 12(-10)\n",
      "Solving the presolved LP\n",
      "Using EKK dual simplex solver - serial\n",
      "  Iteration        Objective     Infeasibilities num(sum)\n",
      "          0     0.0000000000e+00 Pr: 3(18) 0s\n",
      "          5     1.0000000000e+01 Pr: 0(0) 0s\n",
      "Solving the original LP from the solution after postsolve\n",
      "Model   status      : Optimal\n",
      "Simplex   iterations: 5\n",
      "Objective value     :  1.0000000000e+01\n",
      "HiGHS run time      :          0.01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JuMP.optimize!(det)\n",
    "@test JuMP.objective_value(det) == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------\n",
      "         SDDP.jl (c) Oscar Dowson and contributors, 2017-23\n",
      "-------------------------------------------------------------------\n",
      "problem\n",
      "  nodes           : 2\n",
      "  state variables : 1\n",
      "  scenarios       : 2.00000e+00\n",
      "  existing cuts   : false\n",
      "options\n",
      "  solver          : serial mode\n",
      "  risk measure    : SDDP.Expectation()\n",
      "  sampling scheme : SDDP.InSampleMonteCarlo\n",
      "subproblem structure\n",
      "  VariableRef                             : [6, 6]\n",
      "  AffExpr in MOI.GreaterThan{Float64}     : [1, 1]\n",
      "  AffExpr in MOI.LessThan{Float64}        : [1, 1]\n",
      "  VariableRef in MOI.EqualTo{Float64}     : [1, 1]\n",
      "  VariableRef in MOI.GreaterThan{Float64} : [4, 4]\n",
      "  VariableRef in MOI.LessThan{Float64}    : [1, 2]\n",
      "numerical stability report\n",
      "  matrix range     [1e+00, 1e+00]\n",
      "  objective range  [1e+00, 5e+00]\n",
      "  bounds range     [8e+00, 8e+00]\n",
      "  rhs range        [6e+00, 6e+00]\n",
      "-------------------------------------------------------------------\n",
      " iteration    simulation      bound        time (s)     solves  pid\n",
      "-------------------------------------------------------------------\n",
      "         5   2.000000e+01  1.000000e+01  4.726000e+00        25   1\n",
      "        10   0.000000e+00  1.000000e+01  4.849000e+00        50   1\n",
      "-------------------------------------------------------------------\n",
      "status         : iteration_limit\n",
      "total time (s) : 4.849000e+00\n",
      "total solves   : 50\n",
      "best bound     :  1.000000e+01\n",
      "simulation ci  :  1.000000e+01 ± 6.533333e+00\n",
      "numeric issues : 0\n",
      "-------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SDDP.train(model, iteration_limit = 10, log_frequency = 5)\n",
    "@test SDDP.calculate_bound(model) == 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hydro-thermal scheduling\n",
    "\n",
    "Copy-paste from https://odow.github.io/SDDP.jl/latest/examples/Hydro_thermal/#Hydro-thermal-scheduling\n",
    "\n",
    "### Problem Description\n",
    "\n",
    "In a hydro-thermal problem, the agent controls a hydro-electric generator and reservoir. Each time period, they need to choose a generation quantity from thermal $g_t$, and hydro $g_h$, in order to meet demand $w_d$, which is a stagewise-independent random variable. The state variable, $x$, is the quantity of water in the reservoir at the start of each time period, and it has a minimum level of 5 units and a maximum level of 15 units. We assume that there are 10 units of water in the reservoir at the start of time, so that $x_0$ = 10. The state-variable is connected through time by the water balance constraint:\n",
    "$$x.out = x.in - g_h - s + w_i,$$\n",
    "where $x.out$ is the quantity of water at the end of the time period, $x.in$ is the quantity of water at the start of the time period, $s$ is the quantity of water spilled from the reservoir, and $w_i$ is a stagewise-independent random variable that represents the inflow into the reservoir during the time period.\n",
    "\n",
    "We assume that there are three stages, $t$=1, 2, 3, representing summer-fall, winter, and spring, and that we are solving this problem in an infinite-horizon setting with a discount factor of 0.95.\n",
    "\n",
    "In each stage, the agent incurs the cost of spillage, plus the cost of thermal generation. We assume that the cost of thermal generation is dependent on the stage $t$ = 1, 2, 3, and that in each stage, $w$ is drawn from the set $(w_i, w_d)$ = $\\{(0, 7.5), (3, 5), (10, 2.5)\\}$ with equal probability.\n",
    "\n",
    "### Constructing the policy graph\n",
    "\n",
    "There are three stages in our problem, so we construct a linear policy graph with three stages using `SDDP.LinearGraph`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Root\n",
       " 0\n",
       "Nodes\n",
       " 1\n",
       " 2\n",
       " 3\n",
       "Arcs\n",
       " 0 => 1 w.p. 1.0\n",
       " 1 => 2 w.p. 1.0\n",
       " 2 => 3 w.p. 1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = SDDP.LinearGraph(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, because we want to solve an infinite-horizon problem, we add an additional edge between node 3 and node 1 with probability 0.95:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDDP.add_edge(graph, 3 => 1, 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the model\n",
    "\n",
    "The state variable $x$, constructed by passing the `SDDP.State` tag to @variable is actually a Julia struct with two fields: $x.in$ and $x.out$ corresponding to the incoming and outgoing state variables respectively. Both $x.in$ and $x.out$ are standard JuMP variables. The initial_value keyword provides the value of the state variable in the root node (i.e., $x_0$).\n",
    "\n",
    "Compared to a JuMP model, one key difference is that we use @stageobjective instead of @objective. The `SDDP.parameterize` function takes a list of supports for $w$ and parameterizes the JuMP model `sp` by setting the right-hand sides of the appropriate constraints (note how the constraints initially have a right-hand side of 0). By default, it is assumed that the realizations have uniform probability, but a probability mass vector can also be provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A policy graph with 3 nodes.\n",
       " Node indices: 1, 2, 3\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SDDP.PolicyGraph(\n",
    "    graph,\n",
    "    sense = :Min,\n",
    "    lower_bound = 0.0,\n",
    "    optimizer = HiGHS.Optimizer,\n",
    ") do sp, t\n",
    "    @variable(sp, 5 <= x <= 15, SDDP.State, initial_value = 10)\n",
    "    @variable(sp, g_t >= 0)\n",
    "    @variable(sp, g_h >= 0)\n",
    "    @variable(sp, s >= 0)\n",
    "    @constraint(sp, balance, x.out - x.in + g_h + s == 0)\n",
    "    @constraint(sp, demand, g_h + g_t == 0)\n",
    "    @stageobjective(sp, s + t * g_t)\n",
    "    SDDP.parameterize(sp, [[0, 7.5], [3, 5], [10, 2.5]]) do w\n",
    "        set_normalized_rhs(balance, w[1])\n",
    "        return set_normalized_rhs(demand, w[2])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the policy\n",
    "\n",
    "Once a model has been constructed, the next step is to train the policy. This can be achieved using `SDDP.train`. There are many options that can be passed, but `iteration_limit` terminates the training after the prescribed number of SDDP iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------\n",
      "         SDDP.jl (c) Oscar Dowson and contributors, 2017-23\n",
      "-------------------------------------------------------------------\n",
      "problem\n",
      "  nodes           : 3\n",
      "  state variables : 1\n",
      "  scenarios       : Inf\n",
      "  existing cuts   : false\n",
      "options\n",
      "  solver          : serial mode\n",
      "  risk measure    : SDDP.Expectation()\n",
      "  sampling scheme : SDDP.InSampleMonteCarlo\n",
      "subproblem structure\n",
      "  VariableRef                             : [6, 6]\n",
      "  AffExpr in MOI.EqualTo{Float64}         : [2, 2]\n",
      "  VariableRef in MOI.GreaterThan{Float64} : [5, 5]\n",
      "  VariableRef in MOI.LessThan{Float64}    : [1, 1]\n",
      "numerical stability report\n",
      "  matrix range     [1e+00, 1e+00]\n",
      "  objective range  [1e+00, 3e+00]\n",
      "  bounds range     [5e+00, 2e+01]\n",
      "  rhs range        [2e+00, 1e+01]\n",
      "-------------------------------------------------------------------\n",
      " iteration    simulation      bound        time (s)     solves  pid\n",
      "-------------------------------------------------------------------\n",
      "         1   5.350000e+01  3.995194e+01  1.300001e-02        99   1\n",
      "        39   1.313117e+02  2.323093e+02  1.042000e+00      6501   1\n",
      "        62   3.230430e+02  2.353322e+02  2.047000e+00     10350   1\n",
      "        70   6.345417e+02  2.359463e+02  3.163000e+00     13206   1\n",
      "        84   7.354705e+01  2.362427e+02  4.176000e+00     15936   1\n",
      "       100   1.149521e+02  2.363496e+02  5.184000e+00     18612   1\n",
      "       111   5.000000e-01  2.363902e+02  6.187000e+00     20973   1\n",
      "       122   4.815137e+02  2.364162e+02  7.349000e+00     23586   1\n",
      "       133   2.145123e+02  2.364236e+02  8.364000e+00     25611   1\n",
      "       140   1.065504e+03  2.364289e+02  9.426000e+00     27528   1\n",
      "       179   1.344000e+03  2.364356e+02  1.470200e+01     36309   1\n",
      "       200   7.050010e+01  2.364364e+02  1.922100e+01     42264   1\n",
      "-------------------------------------------------------------------\n",
      "status         : iteration_limit\n",
      "total time (s) : 1.922100e+01\n",
      "total solves   : 42264\n",
      "best bound     :  2.364364e+02\n",
      "simulation ci  :  2.009628e+02 ± 2.881329e+01\n",
      "numeric issues : 0\n",
      "-------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SDDP.train(model, iteration_limit = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating the policy\n",
    "\n",
    "After training, we can simulate the policy using `SDDP.simulate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On average, 2.0 units of thermal are used in the first stage.\n"
     ]
    }
   ],
   "source": [
    "sims = SDDP.simulate(model, 100, [:g_t])\n",
    "mu = round(mean([s[1][:g_t] for s in sims]), digits = 2)\n",
    "println(\"On average, $(mu) units of thermal are used in the first stage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the water values\n",
    "\n",
    "Finally, we can use `SDDP.ValueFunction` and `SDDP.evaluate` to obtain and evaluate the value function at different points in the state-space. Note that since we are minimizing, the price has a negative sign: each additional unit of water leads to a decrease in the the expected long-run cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(233.54927464089306, Dict(:x => -0.6583315402827015))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = SDDP.ValueFunction(model[1])\n",
    "cost, price = SDDP.evaluate(V, x = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia nteract 1.9.3",
   "language": "julia",
   "name": "julia-nteract-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
